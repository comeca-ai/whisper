# Whisper FastAPI Endpoint

API REST para transcri칞칚o de 치udio usando OpenAI Whisper.

## 游 In칤cio R치pido

### Instala칞칚o

```bash
# Instalar depend칡ncias da API
pip install -r requirements-api.txt
```

### Executar o servidor

```bash
# Desenvolvimento (com reload autom치tico)
uvicorn api:app --reload --host 0.0.0.0 --port 8000

# Produ칞칚o
uvicorn api:app --host 0.0.0.0 --port 8000 --workers 4
```

O servidor estar치 dispon칤vel em: `http://localhost:8000`

### Documenta칞칚o Interativa

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

## 游니 Endpoints

### GET `/`
Informa칞칫es sobre a API.

### GET `/health`
Health check do servi칞o.

### GET `/models`
Lista os modelos Whisper dispon칤veis.

### POST `/transcribe`
Transcreve um arquivo de 치udio.

**Par칙metros:**
- `file` (obrigat칩rio): Arquivo de 치udio (mp3, wav, m4a, etc.)
- `model` (opcional): Tamanho do modelo (`tiny`, `base`, `small`, `medium`, `large`)
- `language` (opcional): C칩digo do idioma (ex: `pt`, `en`)
- `task` (opcional): `transcribe` ou `translate`
- `temperature` (opcional): Temperatura de amostragem (padr칚o: 0.0)
- `verbose` (opcional): Habilitar sa칤da detalhada

**Exemplo usando curl:**

```bash
curl -X POST "http://localhost:8000/transcribe" \
  -F "file=@audio.mp3" \
  -F "model=base" \
  -F "language=pt"
```

**Exemplo usando Python:**

```python
import requests

with open('audio.mp3', 'rb') as f:
    files = {'file': f}
    data = {'model': 'base', 'language': 'pt'}
    response = requests.post('http://localhost:8000/transcribe', 
                           files=files, data=data)
    
print(response.json()['text'])
```

### POST `/transcribe-simple`
Endpoint simplificado para transcri칞칚o r치pida (usa configura칞칫es padr칚o).

**Par칙metros:**
- `file` (obrigat칩rio): Arquivo de 치udio

## 游냡 Docker

### Build e execu칞칚o

```bash
# Build da imagem
docker build -t whisper-api .

# Executar container
docker run -p 8000:8000 whisper-api
```

### Usando Docker Compose

```bash
docker-compose up
```

## 游빍 Testes

```bash
# Testar a API com um arquivo de 치udio
python test_api.py caminho/para/audio.mp3
```

## 游늵 Modelos Dispon칤veis

| Modelo | Par칙metros | Mem칩ria VRAM | Velocidade Relativa |
|--------|-----------|--------------|---------------------|
| tiny   | 39 M      | ~1 GB        | ~32x                |
| base   | 74 M      | ~1 GB        | ~16x                |
| small  | 244 M     | ~2 GB        | ~6x                 |
| medium | 769 M     | ~5 GB        | ~2x                 |
| large  | 1550 M    | ~10 GB       | 1x                  |

## 游깷 Formatos de 츼udio Suportados

A API suporta qualquer formato que o FFmpeg consiga processar:
- MP3
- WAV
- M4A
- FLAC
- OGG
- AAC
- WMA
- E muitos outros...

## 丘뙖잺 Vari치veis de Ambiente

```bash
# Opcional: configurar device (cpu ou cuda)
export WHISPER_DEVICE=cpu

# Opcional: diret칩rio de cache dos modelos
export WHISPER_CACHE_DIR=/caminho/para/cache
```

## 游닇 Resposta da API

```json
{
  "text": "Texto completo transcrito do 치udio.",
  "language": "pt",
  "segments": [
    {
      "id": 0,
      "start": 0.0,
      "end": 3.5,
      "text": "Primeiro segmento do 치udio."
    },
    {
      "id": 1,
      "start": 3.5,
      "end": 7.2,
      "text": "Segundo segmento do 치udio."
    }
  ]
}
```

## 游 Seguran칞a

Para produ칞칚o, considere:
- Limitar o tamanho m치ximo de upload
- Adicionar autentica칞칚o (API key, OAuth, etc.)
- Configurar rate limiting
- Usar HTTPS
- Validar tipos de arquivo

## 游늳 Performance

Para melhorar a performance:
- Use GPU quando dispon칤vel (CUDA)
- Ajuste o n칰mero de workers do uvicorn
- Considere usar modelos menores (tiny/base) para transcri칞칫es r치pidas
- Implemente cache de resultados para arquivos frequentes
- Use async workers

## 游냍 Solu칞칚o de Problemas

### Erro: "Could not load library cudnn_cnn_infer64_8.dll"
- Execute com CPU: configure a vari치vel de ambiente `WHISPER_DEVICE=cpu`

### Erro: "ffmpeg not found"
- Instale ffmpeg: `apt-get install ffmpeg` (Linux) ou `brew install ffmpeg` (Mac)

### API lenta
- Use modelo menor (tiny ou base)
- Verifique se est치 usando GPU
- Aumente o n칰mero de workers
